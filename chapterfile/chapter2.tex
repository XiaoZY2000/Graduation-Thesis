\chapter{Preliminaries}\label{chap:preliminaries}

This chapter introduces the fundamental concepts and mathematical formulations
that underpin our proposed method, including recommender systems,
cross-domain recommendation, Gaussian mixture models, and optimal transport.
These preliminaries provide the necessary background for understanding the
distributional preference modeling and cross-domain alignment techniques
presented in later chapters.

\section{Recommender Systems}

A recommender system aims to predict users' preferences for items and to provide
personalized item rankings.
Formally, let $U$ denote the set of users and $I$ denote the set of items.
A recommender system can be modeled as a function
\[
R: U \times I \rightarrow S,
\]
where $S$ is the space of possible scores or ratings.
Given a user $u \in U$ and an item $i \in I$, the system predicts a score
$\hat{s}_{ui} = R(u, i)$ that reflects the user's preference for the item.

Based on the predicted scores, the recommender system produces a ranked list of
items for each user.
The primary objective is to rank items such that those with higher predicted
utility are recommended earlier, thereby maximizing user satisfaction and
engagement.

\section{Cross-Domain Recommendation}

Cross-domain recommendation extends the standard recommendation setting by
leveraging information from one or more source domains to improve recommendation
performance in a target domain.
Let $D_s = (U_s, I_s)$ denote a source domain with user set $U_s$ and item set
$I_s$, and let $D_t = (U_t, I_t)$ denote a target domain with user set $U_t$ and
item set $I_t$.
The goal of cross-domain recommendation is to learn a target-domain recommendation
function
\[
R_t: U_t \times I_t \rightarrow S
\]
by exploiting auxiliary information available in the source domain $D_s$.

The transferable information across domains may include user--item interaction
data, user or item attributes, textual reviews, or latent representations learned
from the source domain.
A central challenge in cross-domain recommendation lies in effectively transferring
useful knowledge while accounting for domain heterogeneity, data sparsity, and
cold-start issues.
In particular, many existing methods rely on overlapping users or items to
establish correspondences between domains, an assumption that does not always
hold in real-world applications.

\section{Gaussian Mixture Models}

Gaussian Mixture Models (GMMs) are probabilistic models that represent complex
distributions as weighted combinations of multiple Gaussian components.
A GMM with $K$ components defines a probability density function of the form
\[
p(x) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(x \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k),
\]
where $\pi_k$ denotes the mixture weight of the $k$-th component, satisfying
$\pi_k \ge 0$ and $\sum_{k=1}^{K} \pi_k = 1$.
Each component $\mathcal{N}(x \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$ is a
Gaussian distribution parameterized by a mean vector $\boldsymbol{\mu}_k$ and a
covariance matrix $\boldsymbol{\Sigma}_k$.

GMMs are capable of modeling multi-modal and heterogeneous data distributions,
making them well suited for representing complex phenomena such as diverse and
multi-faceted user preferences.
In this thesis, GMMs are used to model user preferences as probability
distributions rather than single point estimates.

\section{Optimal Transport and Wasserstein Distance}

Optimal transport (OT) is a mathematical framework for comparing probability
distributions by explicitly accounting for the cost of transporting probability
mass between them.
Given two probability measures $\mu$ and $\nu$ defined on spaces $X$ and $Y$,
respectively, and a ground cost function $c(x,y)$ that measures the cost of moving
mass from $x \in X$ to $y \in Y$, the Kantorovich formulation of optimal transport
is defined as
\[
\inf_{\gamma \in \Pi(\mu, \nu)} \int_{X \times Y} c(x, y) \, d\gamma(x, y),
\]
where $\Pi(\mu, \nu)$ denotes the set of all joint probability measures whose
marginals are $\mu$ and $\nu$.

A widely used metric derived from optimal transport is the Wasserstein distance.
For $p \ge 1$, the $p$-Wasserstein distance between $\mu$ and $\nu$ is defined as
\[
W_p(\mu, \nu) =
\left(
\inf_{\gamma \in \Pi(\mu, \nu)}
\int_{X \times Y} c(x, y)^p \, d\gamma(x, y)
\right)^{1/p}.
\]
Unlike conventional pointwise distances such as the Euclidean distance, the
Wasserstein distance captures the underlying geometric structure of probability
distributions and provides meaningful comparisons even when the distributions
have limited or no overlapping support.

In this thesis, we adopt the Wasserstein distance as a fundamental tool for
measuring discrepancies between distributional user preference representations
across domains.
By operating at the distribution level, optimal transport enables principled
cross-domain alignment that is particularly suitable for non-overlapping
recommendation settings.