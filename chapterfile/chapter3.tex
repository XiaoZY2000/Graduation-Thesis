\chapter{Related Work} \label{chap:related_work}

\section{Recommender Systems}
Recommender systems aim to alleviate information overload by modeling user preferences and predicting the relevance of items from large candidate sets. 
Over the past decades, a wide range of recommendation techniques have been proposed, which can be broadly categorized into collaborative filtering-based methods, 
content-based methods, and hybrid approaches. These paradigms differ in how user preferences are represented, how item relevance is estimated, and how auxiliary information is exploited. 
Figure~\ref{fig:recommender_systems} presents a conceptual overview of recommender systems, illustrating how different sources of user- and item-side signals are leveraged by various modeling paradigms to produce recommendation outputs.
In this section, we review representative recommender system techniques that form the foundation of modern recommendation models.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/recommender_systems.pdf}
	\caption{A Conceptual Overview of Recommender Systems}
	\label{fig:recommender_systems}
\end{figure}

\subsection{Collaborative Filtering}

Collaborative Filtering (CF) is one of the most widely studied paradigms in recommender systems~\cite{Bobadilla_Ortega_Hernando_Gutiérrez_2013_recommender_survey,Su_Khoshgoftaar_2009_CF_survey}. 
CF methods rely solely on historical user-item interaction data to infer user preferences, under the assumption that users with similar interaction patterns tend to exhibit similar tastes. 
By relying solely on user--item interaction data,
CF techniques are particularly effective in capturing collective behavioral patterns 
and have been extensively applied in rating prediction and top-$K$ recommendation tasks.

\subsubsection{Memory-based CF}
Memory-based collaborative filtering (CF) methods generate recommendations
directly from observed user--item interactions by exploiting similarity
relationships among users or items.
Depending on whether similarity is computed between users or items, these
methods are commonly divided into user-based and item-based approaches.
User-based CF recommends items based on the preferences of similar users, while
item-based CF suggests items similar to those the user has previously interacted
with.

GroupLens~\cite{Resnick_1994_grouplens} is one of the earliest and most influential memory-based CF systems, 
which introduced user-based collaborative filtering using Pearson correlation to compute user similarities. 
It demonstrated the effectiveness of CF in providing personalized recommendations and laid the groundwork for subsequent research in the field.

Sarwar et al.~\cite{Sarwar_2001_item_based_CF} points out that when the number of users and items is very large, 
user-based CF can be computationally expensive and the sparsity of the user-item interaction matrix can lead to poor recommendation quality. 
Noticing that users' preferences change quickly over time but items' characteristics are relatively stable, they proposed an item-based CF approach that computes item similarities based on user interactions. 
The merit of this method is that item similarities can be precomputed and stored, allowing for efficient recommendation generation. 
And the number of items is usually much smaller than the number of users, which helps alleviate the data sparsity issue. 
Also, one important attribution of this paper is that it proposed a new similarity measure called adjusted cosine similarity, which accounts for individual user rating biases when computing item similarities. 
This method has since become a standard technique in item-based CF and has been widely adopted in various recommender systems.

\subsubsection{Probabilistic Latent Factor Models}

Early collaborative filtering methods are dominated by probabilistic latent factor models, 
which represent users and items as low-dimensional latent vectors and model observed interactions through their inner products. 
These models typically assume that user preferences and item characteristics are generated from latent variables, 
and interaction data are modeled via probabilistic distributions. 
Representative approaches include probabilistic matrix factorization and its extensions, 
which offer interpretable formulations and solid theoretical foundations for preference modeling.

Breese et al.~\cite{Breese_Heckerman_Kadie_2013_CF_analysis} conducted a comprehensive analysis of both memory-based and model-based CF methods. 
This paper is the first to systematically distinguish between memory-based and model-based CF approaches, providing a detailed comparison of their strengths and weaknesses. 
The model-based methods mentioned in this paper include Bayesian Clustering and Bayesian Networks. Bayesian Clustering groups users into clusters based on their preferences, 
while Bayesian Networks model the probabilistic relationships between users and items.

Ungar and Foster~\cite{Ungar_Foster_1998_clustering_CF} pointed out that traditional clustering-based collaborative filtering methods suffer from instability and poor generalization when interaction data are highly sparse, 
as approaches based on KNN or simple K-means clustering rely heavily on local similarity patterns.
To address this issue, they proposed Gibbs clustering for collaborative filtering, a probabilistic co-clustering approach that jointly clusters users and items into latent classes and models their interactions through class-level link probabilities. 
By explicitly formulating a generative model and employing Gibbs sampling for inference, their method enforces global consistency in user and item assignments and provides a principled alternative to heuristic clustering.

Koren et al.~\cite{Koren_Bell_Volinsky_2009_MF_recommender} systematically review matrix factorization techniques for recommender systems, demonstrating that latent factor models with bias, implicit feedback, and temporal dynamics achieve consistently superior accuracy and scalability over neighborhood-based methods, and establishing matrix factorization as a dominant model-based collaborative filtering paradigm.

Salakhutdinov and Mnih introduce Probabilistic Matrix Factorization as a scalable latent factor model for large, sparse recommender systems, and further extend it to a fully Bayesian framework using MCMC, significantly improving robustness and generalization—especially for infrequent users—while establishing PMF as a foundational model-based collaborative filtering paradigm~\cite{Mnih_Salakhutdinov_2007_PMF,Salakhutdinov_Mnih_2008_Bayesian_PMF}.

\subsubsection{Deep Learning-based CF}

With the advancement of deep learning, neural network-based collaborative filtering methods have been proposed to overcome the limited expressiveness of linear latent factor models. 
These approaches leverage multilayer perceptrons, autoencoders, or neural interaction functions to capture complex, nonlinear user-item relationships. 
By learning high-capacity representations from interaction data, deep CF models have achieved strong empirical performance and become a dominant paradigm in modern recommender systems.

\textbf{Neural Interaction Modeling.}
Neural interaction modeling methods extend traditional latent factor approaches by learning flexible, 
non-linear user-item interaction functions through neural networks. 
Rather than assuming a fixed inner product in the latent space, these models parameterize interaction mechanisms using multi-layer architectures, 
enabling richer expressiveness and improved modeling capacity. 
By directly learning interaction patterns from data, neural interaction models overcome the representational limitations of linear factorization methods and form a foundational branch of deep learning-based collaborative filtering.

He et al.~\cite{He_Liao_Zhang_Nie_Hu_Chua_2017_NCF} proposed the Neural Collaborative Filtering (NCF) framework, which formulates collaborative filtering as a neural interaction learning problem.
Instead of relying on a fixed inner product as in traditional matrix factorization, NCF employs neural networks to learn flexible and non-linear user-item interaction functions directly from data.
In this framework, users and items are embedded into low-dimensional latent spaces and their representations are combined through neural architectures—such as generalized matrix factorization (GMF), 
multi-layer perceptrons (MLP), and their fusion model NeuMF—to capture complex interaction patterns beyond linear similarity measures.

\textbf{Sequential Recommendation.}
Sequential recommendation focuses on modeling the temporal order and sequential dependencies within users' interaction histories to capture the dynamic evolution of user preferences. 
By explicitly incorporating sequence information, these methods aim to predict future user behaviors based on past interaction patterns rather than treating interactions as independent events. 
Sequential recommender systems have become a core component of modern recommendation pipelines, 
particularly in scenarios where user preferences exhibit strong temporal dynamics.

Hidasi et al.~\cite{Hidasi_Karatzoglou_Baltrunas_Tikk_2015_session_based_RNN} proposed a session-based recommender system that applies recurrent neural networks (RNNs) to model user behavior within individual sessions.
By representing a session as a sequence of item interactions and maintaining a recurrent hidden state, their approach captures both short-term and long-term dependencies in session data.
Furthermore, the authors introduced ranking-oriented loss functions tailored to recommendation tasks, enabling the model to significantly outperform traditional item-to-item and neighborhood-based baselines.

The mainstream sequential recommender systems now are Transformer-based models. Kang and McAuley~\cite{Kang_McAuley_2018_SASRec} proposed SASRec, a self-attentive sequential recommender system based on the Transformer encoder architecture.
By employing self-attention mechanisms, SASRec adaptively weighs historical items in a user's interaction sequence, enabling the model to capture long-range dependencies while remaining efficient on sparse data.
Unlike recurrent models that summarize sequences through a single hidden state, SASRec directly attends to relevant past interactions, leading to improved recommendation accuracy and scalability in sequential recommendation tasks.

Sun et al.~\cite{Sun_Yuan_Wang_Shen_Li_Lu_2019_BERT4Rec} proposed BERT4Rec, a Transformer-based sequential recommender system that employs bidirectional self-attention to model user behavior sequences.
Unlike unidirectional sequential models such as RNN-based methods and SASRec, BERT4Rec leverages bidirectional contextual information by predicting masked items within a sequence using a Cloze-style training objective.
This design enables each item representation to incorporate both preceding and succeeding context, leading to more expressive sequence modeling and consistently improved recommendation performance across multiple benchmark datasets.

\textbf{Graph-based Collaborative Filtering.}
Graph-based collaborative filtering models user-item interactions as graph-structured data and leverages graph neural networks to learn representations through neighborhood aggregation. 
By propagating information along interaction edges, these methods capture high-order collaborative signals that are difficult to model with point-wise interaction functions. 
Graph-based frameworks also naturally support the incorporation of side information and heterogeneous relations, enabling more expressive modeling of user preferences and item characteristics.

Van den Berg et al.~\cite{VanDenBerg_Thomas_Kipf_Welling_2017_GCMC} proposed Graph Convolutional Matrix Completion (GCMC), which formulates collaborative filtering as a link prediction problem on a bipartite user-item interaction graph.
By employing a graph convolutional auto-encoder architecture, GCMC learns user and item representations through message passing on the interaction graph and reconstructs ratings via a bilinear decoder.
This approach effectively captures high-order collaborative signals and naturally incorporates side information, leading to improved recommendation performance on benchmark datasets.

Ying et al.~\cite{Ying_He_Chen_Eksombatchai_Hamilton_Leskovec_2018_GCN_recommender} proposed PinSage, a graph-based recommender system designed for web-scale applications.
PinSage combines graph neural networks with efficient random-walk-based neighborhood sampling to learn item representations that incorporate both graph structure and rich side information.
By addressing the scalability limitations of conventional GCNs, PinSage was successfully deployed in large-scale industrial systems such as Pinterest, demonstrating the effectiveness of graph-based recommendation models in real-world production environments.

Wang et al.~\cite{Wang_He_Wang_Feng_Chua_2019_NGCF} proposed Neural Graph Collaborative Filtering (NGCF), which explicitly integrates graph neural networks into collaborative filtering by modeling user-item interactions as a bipartite graph.
NGCF refines user and item embeddings through recursive message passing on the interaction graph, enabling the explicit modeling of high-order connectivity and collaborative signals, which leads to significant improvements in recommendation performance.

He et al.~\cite{He_Deng_Wang_Li_Zhang_Wang_2020_LightGCN} introduced LightGCN, a simplified graph convolutional network tailored for recommender systems.
Unlike prior GNN-based models that incorporate feature transformations and nonlinear activations, LightGCN argues that these components contribute little to collaborative filtering performance when only user and item IDs are available as input.
Accordingly, LightGCN retains only the neighborhood aggregation operation to propagate embeddings over the user-item interaction graph, significantly simplifying the model architecture while preserving the ability to capture high-order connectivity.
Extensive experiments demonstrate that LightGCN not only reduces computational complexity but also achieves superior recommendation accuracy compared to more complex GNN-based methods such as NGCF, making it a widely adopted and strong baseline in graph-based recommender system research.

Wu et al.~\cite{Wu_Tang_Zhu_Wang_Xie_Tan_2019_SR-GNN} proposed SR-GNN, a session-based recommender system that models user interaction sequences as graph-structured data and applies graph neural networks to learn item representations within sessions.
By constructing a directed session graph for each interaction sequence, SR-GNN is able to capture complex transition patterns among items that go beyond simple sequential dependencies.
This work represents an early attempt to integrate graph-based modeling with sequential recommendation, effectively combining the strengths of GNNs in capturing high-order relational information and sequential models in characterizing short-term user intent.

\textbf{Generative Recommendation Models.}
Generative recommendation models formulate the recommendation task from a probabilistic generative perspective, 
aiming to model the underlying data generation process of user-item interactions. 
Instead of directly optimizing discriminative prediction objectives, these approaches learn likelihood-based or distributional representations that can capture uncertainty and variability in user preferences. 
By enabling principled sampling and probabilistic inference, generative models provide a complementary modeling paradigm that has gained increasing attention in recommender systems research.

Liang et al.~\cite{Liang_Krishnan_Hoffman_Jebara_2018_VAE_CF} proposed a variational autoencoder (VAE)-based collaborative filtering framework for implicit feedback data.
By modeling user preferences as latent random variables and employing a multinomial likelihood, this approach provides a probabilistic formulation that captures uncertainty and multi-modal structures in user behavior.
Compared with deterministic autoencoder-based models, the VAE framework enables more expressive preference modeling and demonstrates strong empirical performance under sparse interaction settings.

Wang et al.~\cite{Wang_Yu_Zhang_Gong_Xu_Wang_Zhang_Zhang_2017_IRGAN} proposed IRGAN, a generative adversarial framework that unifies generative and discriminative models for information retrieval and recommendation tasks.
By formulating the learning process as a minimax game, the generative model aims to approximate the underlying relevance distribution over items, while the discriminative model learns to distinguish relevant from non-relevant user-item pairs.
Through adversarial training, IRGAN effectively improves recommendation performance, particularly under implicit feedback settings.

Chae et al.~\cite{Chae_Kang_Kim_Lee_2018_CFGAN} proposed CFGAN, a generic collaborative filtering framework based on generative adversarial networks.
Unlike prior GAN-based recommender systems that generate discrete item indices, CFGAN adopts vector-wise adversarial training, where the generator produces real-valued preference vectors and the discriminator distinguishes them from ground-truth interaction vectors.
This design effectively stabilizes adversarial learning and improves recommendation accuracy, especially under sparse implicit feedback settings.

Beyond VAEs and GANs, diffusion models have recently been introduced to recommender systems as a new class of generative models.
Wang et al.~\cite{Wang_Xu_Feng_Lin_He_Chua_2023_DiffRec} proposed DiffRec, a diffusion-based recommender system that models the user-item interaction generation process through iterative denoising.
By gradually corrupting user interaction histories and learning to recover the original interactions step by step, DiffRec provides a flexible and expressive framework for modeling complex preference distributions.
This work demonstrates the potential of diffusion models to overcome the limitations of traditional generative approaches and further improve recommendation performance under noisy and sparse interaction settings.

\subsection{Content-based and Hybrid Recommendation Methods}
Content-based recommendation methods exploit item attributes and user profiles to model user preferences based on item content similarity rather than collective behavior. 
While such approaches are effective in cold-start scenarios, they often suffer from limited generalization due to narrow user interest modeling. 
To address the limitations of both collaborative filtering and content-based methods, hybrid recommendation approaches combine interaction data with auxiliary information, aiming to achieve robust performance across diverse recommendation settings.

Salton et al.~\cite{Salton_Buckley_Fox_1983_auto_query_formulation} proposed the vector space model for information retrieval, in which both documents and queries are represented as weighted term vectors.
By assigning importance weights to terms (e.g., inverse document frequency) and computing similarity scores between query and document vectors, the model enables ranked retrieval based on relevance.
This representation and similarity-matching paradigm laid the foundation for content-based recommender systems, where user profiles and item content are similarly modeled in a shared feature space.

Pazzani and Billsus~\cite{Pazzani_Billsus_1997_user_profiles} proposed a content-based recommender system that learns user profiles from explicit user feedback on item content.
Their method represents items using content features and employs a naive Bayesian classifier to incrementally learn and revise user preference profiles, enabling the system to predict the interestingness of unseen items.

He et al.~\cite{He_Pan_Jin_Xu_Liu_Xu_Shi_Atallah_Herbrich_Bowers_2014_facebook_ads} developed a large-scale recommender system for Facebook Ads based on gradient boosting decision trees.
By modeling user preferences from rich user, item, and contextual features, their approach significantly improved ad targeting effectiveness and user engagement, demonstrating the practicality of supervised learning methods in real-world industrial recommendation scenarios.

Furthermore, advanced gradient boosting frameworks such as XGBoost~\cite{Chen_2016_XGBoost} and LightGBM~\cite{Ke_Meng_Finley_Wang_Chen_Ma_Ye_Liu_2017_LightGBM} have been widely applied in recommender systems to enhance both accuracy and scalability.
These methods leverage efficient tree-based boosting strategies to model high-order feature interactions, making them particularly suitable for large-scale recommendation tasks with sparse and high-dimensional feature spaces.

Burke~\cite{Burke_2002_hybrid_recommender} presented a comprehensive survey of hybrid recommender systems, systematically categorizing hybridization strategies such as weighted, switching, mixed, and feature combination approaches.
The survey analyzed how different hybrid designs integrate multiple recommendation techniques to balance their respective strengths and weaknesses, demonstrating that hybrid methods can effectively improve recommendation accuracy and alleviate issues such as data sparsity and cold-start problems.

Pazzani~\cite{Pazzani_1999_framework_CDR} proposed a unified framework for integrating collaborative, content-based, and demographic filtering methods in recommender systems.
By exploiting multiple sources of information, including user–item interactions, item content, and user profiles, the framework combines recommendations from different models to improve precision.
Experimental results demonstrated that hybrid approaches within this framework consistently outperform single-method recommenders.

Melville et al.~\cite{Melville_Mooney_Nagarajan_2002_content_boosted_CF} proposed a content-boosted collaborative filtering (CBCF) framework that integrates content-based prediction into the collaborative filtering process.
Specifically, a content-based predictor is first used to generate pseudo ratings for unrated items, producing a dense pseudo user–item matrix on which collaborative filtering is subsequently applied.
By alleviating sparsity and the first-rater problem, this approach achieves significantly improved recommendation accuracy compared to pure collaborative, pure content-based, and naive hybrid methods.

Billsus et al.~\cite{Billsus_Pazzani_2000_user_modeling} developed a hybrid news recommender system for adaptive news access that integrates collaborative filtering and content-based filtering techniques.
Their system learns personalized user models from both explicit and implicit user feedback, and combines short-term and long-term interest representations to adapt to users’ evolving information needs.
Deployed in a real-world news delivery environment, this work demonstrated the practical effectiveness of hybrid recommender systems in improving personalization quality without requiring additional user effort.

Basilico and Hofmann~\cite{Basilico_Hofmann_2004_unifying_CF_content} proposed a unified supervised learning framework that integrates collaborative filtering and content-based filtering within a single prediction model.
Their approach formulates recommendation as a learning problem over user-item pairs by designing joint feature representations and kernel functions that enable simultaneous generalization across both user and item dimensions.
By incorporating user-item interaction data together with item and user attributes, the framework achieves improved recommendation accuracy compared to traditional collaborative or content-based methods.

Rendle~\cite{Rendle_2010_factorization_machines} introduced Factorization Machines (FMs), a supervised learning model that generalizes matrix factorization by modeling pairwise feature interactions through factorized parameters.
By representing user–item interactions, item attributes, and contextual information as sparse feature vectors, FMs can efficiently capture interactions in high-dimensional and highly sparse settings.
This unified formulation subsumes several state-of-the-art factorization models and has demonstrated superior performance over traditional collaborative and content-based approaches in various recommendation tasks.

Burges et al.~\cite{Burges_Shaked_Renshaw_Lazier_Deeds_Hamilton_Hullender_2005_LTR} proposed a learning-to-rank framework that directly optimizes the ordering of items rather than predicting absolute preference scores.
Their approach formulates ranking as a pairwise learning problem and introduces RankNet, which models ranking preferences using a probabilistic cost function optimized via gradient descent.
By focusing on ranking quality, this framework significantly improves recommendation effectiveness in scenarios where the relative order of items is more important than precise rating prediction.

Later, Burges~\cite{Burges_2010_ranknet_lambdarank_lambdamart} provided a comprehensive overview of learning-to-rank methods, including RankNet, LambdaRank, and LambdaMART.
RankNet formulates ranking as a pairwise probabilistic learning problem optimized via gradient descent, while LambdaRank introduces the concept of lambda gradients to directly optimize ranking metrics such as NDCG.
By combining LambdaRank with gradient-boosted decision trees, LambdaMART further improves ranking performance and has become a widely adopted approach in large-scale recommendation and information retrieval systems.

\section{Cross-Domain Recommender Systems}

Despite the success of single-domain recommender systems, challenges such as data sparsity and cold-start users remain difficult to address when interaction data are limited.
Cross-Domain Recommendation (CDR) tackles these issues by transferring knowledge from a source domain with abundant user-item interactions to a target domain where data are scarce.

The core objective of CDR is to leverage auxiliary information across domains to improve recommendation performance, under the assumption that user preferences or item characteristics exhibit certain transferable patterns.
By exploiting correlations between domains, CDR methods aim to enhance recommendation accuracy and robustness, especially in cold-start and sparse-data scenarios.

Based on whether domains share common entities, existing CDR approaches can be broadly categorized into \emph{overlapping} and \emph{non-overlapping} settings.
Overlapping CDR methods assume the existence of shared users or items across domains and utilize this overlap as a bridge for knowledge transfer in the training stage.
Typical techniques include joint matrix factorization, co-clustering, and graph-based models that align user preferences or item representations across domains.

In contrast, non-overlapping CDR methods address more challenging scenarios where no users or items are shared between domains during training.
These approaches generally rely on content features, latent representations, or learned mappings between domains to enable preference transfer without explicit entity overlap.
Non-overlapping CDR is particularly relevant in practical applications, but remains challenging due to the lack of direct correspondence between domains.

Figure~\ref{fig:CDR} illustrates representative cross-domain recommendation
paradigms under different overlap assumptions, highlighting how knowledge
transfer is achieved either through shared entities or via shared latent spaces.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/CDR.pdf}
	\caption{Conceptual comparison between overlapping and non-overlapping cross-domain
recommendation paradigms, illustrating how two domains are connected for
knowledge transfer.}
	\label{fig:CDR}
\end{figure}

\subsection{Overlapping CDR}
Overlapping cross-domain recommendation assumes the existence of shared entities, 
such as users or items, across domains. 
These shared entities provide explicit bridges that enable direct knowledge transfer 
through shared latent representations or alignment constraints. 
Many existing methods exploit overlapping users or items to jointly learn cross-domain embeddings, 
often using multi-task learning, graph-based modeling, or contrastive objectives to enforce consistency across domains.

Singh and Gordon~\cite{Singh_Gordon_2008_CMF} proposed Collective Matrix Factorization (CMF), a multi-relational matrix factorization framework that jointly factorizes multiple user–item interaction matrices across domains.
By sharing latent factors for overlapping users or items, CMF enables effective knowledge transfer between related domains and improves recommendation performance under data sparsity.
This work is widely regarded as a foundational approach for overlapping cross-domain recommendation.

Li et al.~\cite{Li_Yang_Xue_2009_transfer_learning_CF} proposed a transfer learning framework for collaborative filtering based on a rating-matrix generative model.
Their approach captures shared latent rating patterns across domains by learning a common cluster-level rating structure, enabling knowledge transfer even under severe data sparsity.
This framework provides a principled probabilistic formulation for overlapping cross-domain recommendation and significantly improves recommendation performance in the target domain.

Pan et al.~\cite{Pan_Xiang_Liu_Yang_2010_transfer_learning_CF} proposed a transfer learning framework for collaborative filtering that alleviates data sparsity by transferring knowledge from auxiliary domains.
Their method discovers shared latent structures of users and items in auxiliary data through matrix factorization and adapts these structures to the target domain via a principled regularization scheme.
By exploiting overlapping users or items across domains, this approach effectively captures transferable preference patterns and improves recommendation performance in sparse target domains.

Zhu et al.~\cite{Zhu_Jin_Zhang_Meng_Zhang_Li_2024_WCDR} proposed WCDR, a dual-target cross-domain recommendation framework that models users and items as probability distributions to explicitly capture uncertainty across domains.
Unlike prior embedding-based overlapping CDR methods that rely on deterministic representations, WCDR represents user and item behaviors as elliptical Gaussian distributions and decouples them into local-intrinsic and global-domain components.
To facilitate effective knowledge transfer between overlapping domains, WCDR further adopts a shared-specific paradigm for global-domain distributions and introduces a Masked Domain-aware Sub-distribution Aggregation (MDSA) module in the Wasserstein space.
This design enables adaptive aggregation of domain-shared knowledge while alleviating negative transfer caused by domain imbalance.
Extensive experiments demonstrate that modeling domain-level uncertainty and distributional relationships can significantly improve recommendation performance in both source and target domains.

\subsection{Non-overlapping CDR}
In contrast, non-overlapping cross-domain recommendation addresses a more challenging setting where no shared users or items exist between domains. 
Without explicit entity-level correspondences, knowledge transfer must rely on higher-level structural, semantic, or distributional similarities across domains. 
This setting better reflects real-world scenarios involving independent platforms or privacy constraints, and has recently attracted increasing research attention due to its practical significance and methodological difficulty.

Man et al.~\cite{Man_Shen_Jin_Cheng_2017_CDR_embedding_mapping} proposed EMCDR, an
early embedding-and-mapping framework for non-overlapping cross-domain
recommendation.
EMCDR first learns latent user and item representations independently in each
domain and then transfers preferences by learning a mapping function between
embedding spaces, enabling cross-domain recommendation under non-overlapping
and cold-start scenarios.

Beyond embedding mapping, several works leverage auxiliary semantic features as
implicit bridges between domains.
Tang et al.~\cite{Tang_Wang_Zhang_2018_TDAR} proposed TDAR, a text-driven
adversarial recommendation framework that exploits review text information to
learn domain-invariant user and item representations.
By aligning representations at the semantic level rather than relying on shared
entities, TDAR enables effective knowledge transfer under strict non-overlapping
settings.

From a feature-level perspective, Kusano and Oyamada~\cite{Kusano_Oyamada_2023_ATP}
formulated non-overlapping cross-domain recommendation as an attribute
transportation problem.
ATP aligns heterogeneous attribute spaces across domains using optimal
transport, allowing users to be re-described in a unified attribute space for
cross-domain similarity estimation and recommendation.
This attribute-level alignment paradigm provides an interpretable and
feature-driven alternative to latent embedding mapping methods.

More recent studies explore non-overlapping CDR from distribution-aware
perspectives by moving beyond static embedding mapping.
Liu et al.~\cite{Liu_Chen_Liao_Hu_Tan_Wang_Zheng_Ong_2024_JPEDET} proposed JPEDET, a
joint preference exploration and dynamic embedding transportation framework for
non-overlapping cross-domain recommendation.
JPEDET leverages both rating and review information to learn transferable user
representations within each domain, and models cross-domain adaptation as a
continuous embedding transportation process.
By enabling dynamic and bidirectional transformation between source and target
embedding spaces, JPEDET captures richer cross-domain preference shifts than
static or adversarial alignment methods.

From a complementary distributional perspective, Liu et
al.~\cite{Li_Qiu_Zhao_Wang_Zhang_Xing_Wu_2022_GWCDR} proposed GWCDR, which performs
cross-domain knowledge transfer by aligning user--item interaction pattern
distributions.
Instead of explicitly mapping individual user or item embeddings, GWCDR
constructs interaction pattern distributions and employs
Gromov--Wasserstein optimal transport to preserve the relational geometry of
interactions across domains.
This relational distribution alignment paradigm enables effective knowledge
transfer under strict non-overlapping settings and provides a distinct
alternative to embedding- or attribute-level alignment approaches.

\section{Distributional User Preference Modeling}

Recent studies have recognized that user preferences are often uncertain and multi-modal, reflecting diverse and evolving interests.
However, most traditional recommender systems represent user preferences as deterministic point embeddings in a latent space, which implicitly assume a single dominant preference pattern.

To better capture preference variability, distributional user preference modeling has been proposed as an alternative paradigm that represents user interests as probability distributions.
By modeling preferences at the distribution level, these approaches provide a more expressive representation that can capture uncertainty, preference diversity, and complex user-item interaction patterns.

\subsection{Multi-interest User Modeling}

Multi-interest user modeling aims to capture the fact that a single user may exhibit multiple distinct preference patterns across different contexts or item categories. 
Instead of learning a single latent representation per user, these methods explicitly model multiple interest vectors or components, allowing recommender systems to better reflect diverse and dynamic user behaviors. 
Such approaches have been shown to improve recommendation accuracy, especially in scenarios with heterogeneous user preferences.

Zhou et al.~\cite{Zhou_Zhu_Song_Fan_Zhu_Ma_Yan_Jin_Li_Gai_2018_DIN} proposed the Deep Interest Network (DIN), which models users’ diverse interests through a target-aware attention mechanism.
Instead of compressing all historical behaviors into a fixed-length representation, DIN dynamically aggregates user behavior embeddings conditioned on the target item, allowing different interests to be activated for different recommendation candidates.
This adaptive representation enables DIN to capture the multi-faceted nature of user preferences and has been shown to achieve strong performance in large-scale industrial recommender systems.

Li et al.~\cite{Li_Liu_Wu_Xu_Zhao_Huang_Kang_Chen_Li_Lee_2019_MIND} proposed the Multi-Interest Network with Dynamic Routing (MIND), which explicitly represents each user with multiple interest vectors.
MIND employs a dynamic routing mechanism to cluster user behaviors into distinct interest representations, enabling the model to capture diverse and heterogeneous user preferences.
By decoupling interest extraction from item matching, MIND is suitable for large-scale retrieval scenarios and has been successfully deployed in industrial recommender systems.

Cen et al.~\cite{Cen_Zhang_Zou_Zhou_Yang_Tang_2020_ComiRec} proposed ComiRec, a controllable multi-interest recommender framework that explicitly models users with multiple interest representations.
ComiRec employs capsule networks or self-attention mechanisms to extract diverse user interests from behavior sequences, and introduces an aggregation module to balance recommendation accuracy and diversity.
By jointly considering multi-interest extraction and controllable aggregation, ComiRec extends prior multi-interest methods and has demonstrated strong effectiveness in large-scale industrial recommender systems.

\subsection{Distribution-based Preference Modeling}

Beyond discrete multi-interest representations, distribution-based preference modeling represents user preferences using continuous probability distributions. 
By modeling uncertainty, variance, and multi-modality in user interests, these approaches provide a richer and more flexible representation framework. 
Distributional modeling techniques, such as Gaussian-based or mixture-based representations, have been explored to capture complex preference structures and improve robustness in sparse or noisy recommendation settings.

Hofmann~\cite{Hofmann_2004_PLSA_recommender} proposed probabilistic latent semantic analysis (PLSA) for collaborative filtering, formulating user–item interactions as a latent class mixture model. In PLSA, each user is associated with a probability distribution over latent topics (or communities), and each interaction is generated by first sampling a latent topic and then drawing an item conditioned on that topic. As a result, user preferences are represented as distributions over latent semantic factors, allowing different interactions of the same user to be explained by different latent causes, rather than being tied to a single latent representation.

Marlin~\cite{Marlin_2003_user_rating_profiles} proposed the User Rating Profile (URP) model, a probabilistic latent variable approach for rating-based collaborative filtering that explicitly models uncertainty in user preferences. URP represents each user as a mixture over latent user attitudes, where the mixture proportions are drawn from a Dirichlet distribution. For each item, a latent attitude is sampled and the corresponding rating is generated according to an attitude-specific rating distribution.
By modeling users as distributions over latent preference patterns rather than fixed point representations, URP enables different items rated by the same user to be explained by different latent factors and allows direct inference of rating distributions for unseen items.

Blei, Ng, and Jordan~\cite{Blei_Ng_Jordan_2003_LDA} proposed Latent Dirichlet Allocation (LDA), a hierarchical generative probabilistic model that represents each document as a mixture over latent topics, where the topic proportions are drawn from a Dirichlet prior. By introducing a document-level latent variable, LDA provides a fully generative framework that enables principled inference for previously unseen data.
Although originally developed for text modeling, LDA has been extended to collaborative filtering by treating users as documents and items as words. Under this formulation, user preferences are modeled as probability distributions over latent topics, allowing each user–item interaction to be explained by different latent factors. This distributional representation enables LDA to capture heterogeneous user interests more flexibly than single-vector latent representations.

Salakhutdinov and Mnih~\cite{Salakhutdinov_Mnih_2008_Bayesian_PMF} further advanced this direction by introducing Bayesian Probabilistic Matrix Factorization (BPMF), which models user and item latent factors as Gaussian distributions and performs Bayesian inference to explicitly account for uncertainty in preference estimation.

VAE-based methods provide another probabilistic framework for modeling uncertainty in user preferences. Liang et al.~\cite{Liang_Krishnan_Hoffman_Jebara_2018_VAE_CF} extended variational autoencoders to collaborative filtering by modeling user preferences as latent random variables and performing variational inference. By representing users with posterior distributions in the latent space, VAE-based recommender systems are able to capture both the variability and uncertainty inherent in user behavior, leading to more robust recommendation performance under sparse interaction data.

Beyond recommender system-specific models, some studies investigate representing entities as probability distributions in general representation learning.
Vilnis and McCallum~\cite{Vilnis_McCallum_2015_Gaussian_embeddings} proposed Gaussian embeddings, which represent words as Gaussian distributions in the embedding space, enabling the modeling of both semantic uncertainty and asymmetric relationships.
Although not originally designed for recommender systems, this distributional representation paradigm provides important methodological insights and has inspired subsequent work on modeling users and items as distributions in recommendation tasks.

\section{Optimal Transport for Representation Alignment}

Optimal Transport (OT) provides a principled mathematical framework for measuring and aligning probability distributions. 
Originally developed in probability theory and operations research, OT has recently gained significant attention in machine learning for representation alignment and domain adaptation. 
By computing minimal-cost transformations between distributions, OT enables structured and interpretable alignment across heterogeneous representation spaces.

\subsection{OT-based Alignment in Cross-Domain Recommendation}
In the context of cross-domain recommendation, OT has been employed to align user or item representations across domains by treating them as probability distributions. 
OT-based alignment offers a natural way to model distribution-level discrepancies while preserving intrinsic geometric structures. 
This property makes OT particularly suitable for cross-domain scenarios, where direct entity correspondences are unavailable or insufficient, and motivates its adoption in recent cross-domain recommendation frameworks.

Villani~\cite{Villani_2008_optimal_transport} systematically developed the theoretical foundations of optimal transport, formalizing the Kantorovich relaxation of the Monge problem and introducing Wasserstein distances as principled metrics between probability measures.
By framing distribution comparison as a cost-minimizing mass transportation problem, this work established a rigorous mathematical basis for measuring and aligning probability distributions, which later enabled the adoption of OT as a core tool in machine learning tasks such as distribution alignment, domain adaptation, and representation learning.

Peyré and Cuturi~\cite{Peyre_Cuturi_2019_computational_OT} provided a comprehensive survey of optimal transport from a computational and machine learning perspective, systematically bridging OT theory with practical algorithms.
Their work reviewed a wide range of OT-based applications, including domain adaptation, generative modeling, and deep learning, and emphasized scalable solutions such as entropic regularization and Sinkhorn iterations.
By demonstrating how OT can be efficiently integrated into modern learning pipelines, this survey established OT as a practical and versatile tool for distribution comparison and representation alignment in machine learning.

Cuturi and Marco~\cite{Cuturi_2013_Sinkhorn_distances} introduced the Sinkhorn algorithm, which incorporates an entropic regularization term into the optimal transport formulation to obtain a smooth and strictly convex objective.
This regularization allows the resulting OT problem to be solved efficiently via iterative matrix scaling, leading to orders-of-magnitude speedups compared to classical linear programming solvers.
As a result, Sinkhorn-based OT distances make optimal transport computationally feasible for large-scale machine learning applications and have become a cornerstone for OT-based representation learning and distribution alignment methods.

Genevay et al.~\cite{Genevay_Peyre_Cuturi_2018_Sinkhorn_generative_models} proposed Sinkhorn divergences as a differentiable and computationally tractable OT-based loss for training generative models.
By introducing entropic regularization and leveraging automatic differentiation through Sinkhorn iterations, their approach enables stable optimization and efficient gradient computation at scale.
This work demonstrated that OT-based losses can effectively improve the quality of generated samples, highlighting the potential of optimal transport as a powerful tool for generative modeling.

Courty et al.~\cite{Courty_Flamary_Tuia_Rakotomamonjy_2016_OT_domain_adaptation} proposed a seminal optimal-transport-based framework for unsupervised domain adaptation, in which feature distributions from the source and target domains are aligned by learning an optimal transportation plan.
By minimizing the Wasserstein distance between empirical distributions, their method explicitly addresses domain shift at the distribution level and enables effective knowledge transfer from the source domain to the target domain.
Moreover, the proposed regularized OT formulation allows the incorporation of class and structural information, leading to improved adaptation performance and establishing OT as a principled tool for representation alignment in domain adaptation.

Frogner et al.~\cite{Frogner_Zhang_Mobahi_Araya_Poggio_2015_Wasserstein_loss} introduced a deep learning framework that incorporates the Wasserstein distance as a loss function for representation learning and domain adaptation.
By embedding OT-based losses into neural networks, their method enables end-to-end training while explicitly accounting for the geometric structure of the output space.
Through an entropically regularized formulation, the proposed Wasserstein loss can be efficiently optimized via gradient-based methods, allowing effective alignment of feature distributions across domains and demonstrating the feasibility of OT within deep learning frameworks.