\chapter{Experiments} \label{chap:experiments}

In this chapter, we present the detailed setup of our experiments to evaluate the
proposed \textbf{DUP-OT} framework for non-overlapping cross-domain recommendation training.
Specifically, we first proposed the research questions we want to solve by conducting experiments,
and then we'll describe the datasets we used and how we preprocessed them, the
experiments we conducted, including the baseline models we compared against, the evaluation metrics we employed.
Finally, we outline the implementation details of our proposed method, including
hyperparameter settings and training procedures to ensure reproducibility.

The final results and analyses will be presented in the next chapter.

\section{Research Questions}
As the proposed \textbf{DUP-OT} framework introduces two key components: (1) distribution-based user preference modeling using GMMs, and (2) cross-domain preference transfer via optimal transport,
we want to design experiments to figure out the effects of these two components respectively.
Moreover, we also want to compare our proposed method with existing non-overlapping cross-domain recommendation methods.
Thus, the research questions we aim to answer through our experiments can be summarized as follows:
\begin{itemize}
	\item \textbf{RQ1:} Does introducing cross-domain information enhance the recommendation performance in the target domain?
	\item \textbf{RQ2:} Does modeling user preferences as distributions, rather than vectors, lead to improved recommendation performance?
	\item \textbf{RQ3:} How does the performance of our proposed method compare with that of existing non-overlapping CDR models?
\end{itemize}

To answer \textbf{RQ1}, we conduct an ablation study comparing target-domain recommendation performance with and without leveraging
source-domain information. When we do not use source-domain data, our model essentially reduces to a single-domain recommendation model that employs GMMs for user preference modeling.

To address \textbf{RQ2}, we evaluate our target-domain model without source-domain information against
some single-domain baseline models that represent user preferences as vectors.
The idea is, if we don't use source-domain data, the target-domain model will reduce to a single-domain recommendation model,
and thus the only difference between our model and the baselines will be the way of user preference modeling.
Thus we can isolate the effect of distribution-based user preference modeling.

Finally, to answer \textbf{RQ3}, we simply compare our full model with representative non-overlapping CDR baseline models
in the same experimental settings.

\section{Datasets and Preprocessing}
We set our experiment under an e-commerce scenario, where different product categories are treated as different domains.
For example, Electronics and Video Games are two different domains, and our target is to leverage user interactions in the source domain (e.g., Video Games)
to improve recommendation performance in the target domain (e.g., Electronics).
To this end, we choose the Amazon Review dataset~\cite{Ni_Li_McAuley_2019_Justifying_Recommendations} for our experiments,
specifically, we choose the 5-core version of four domains: Electronics, Digital Music, Movies \& TV, and Video Games.
To make sure the 5-core really holds in these four datasets, we further filter the datasets to ensure that each user and item has at least 5 interactions.
After preprocessing, the statistics of the four datasets are summarized in Table~\ref{tab:dataset_stats}.

\begin{table}[h]
	\centering
	\caption{Statistics of the Amazon Review datasets after preprocessing.}
	\label{tab:dataset_stats}
	\begin{tabular}{lrrrr}
		\toprule
		Domain        & Users   & Items   & Interactions & Density (\%) \\
		\midrule
		Electronics   & 728,489 & 159,729 & 6,737,580    & 0.0058       \\
		Digital Music & 16,252  & 11,269  & 166,942      & 0.0912       \\
		Movies \& TV  & 297,377 & 59,925  & 3,408,612    & 0.0191       \\
		Video Games   & 55,144  & 17,286  & 496,904      & 0.0521       \\
		\bottomrule
	\end{tabular}
\end{table}

Considering the interaction density and the relative time stamps of the four domains,
we select Electronics as the target domain, and the other three domains as source domains in our experiments.

Although our proposed method is not a sequential recommendation model, we think it's still important to avoid data leakage
when splitting the datasets into training, validation, and test sets. So we sort the interactions of each user by their time stamps,
and then split target-domain interactions, in our case, Electronics, into training, validation, and test sets globally by the ratio of 80\%, 10\%, and 10\%.
Then to avoid data leakage, we need to ensure the interactions in the source domain used for training happened before
the earliest interaction in the target-domain validation set. To this end, we first find the earliest time stamp $t_{val}$ in the target-domain validation set,
and then filter the source-domain interactions to only keep those happened before $t_{val}$.
To reduce computational costs, we search for $t_{val}$ in a step of 5\% in the time-stamp sorted source-domain interactions.
We don't need to split the source-domain interactions into training, validation, and test sets,
as we only use source-domain data for training in our experiments.

As we split the interactions of target-domain globally, it's possible that some users only appear in the validation or test sets.
When we build the Dataset object for training, we just randomly set the embeddings of the users who only appear in the validation or test sets as random vectors.

\section{Experimental Setup}
To answer the research questions proposed above, we mainly design three groups of experiments
corresponding to \textbf{RQ1}, \textbf{RQ2}, and \textbf{RQ3} respectively.
All experiments are conducted on the preprocessed Amazon Review datasets described in the previous section, and
the target domain is always Electronics while the source domain varies among Digital Music, Movies \& TV, and Video Games.
In other words, we conduct experiments on three source-target domain pairs:
\begin{itemize}
	\item Digital Music $\rightarrow$ Electronics
	\item Movies \& TV $\rightarrow$ Electronics
	\item Video Games $\rightarrow$ Electronics
\end{itemize}

To answer \textbf{RQ1}, which investigates the effect of cross-domain information,
we compare two variants of our proposed framework: DUP-OT (w/ source) and DUP-OT (w/o source),
which means we either leverage source-domain information for target-domain recommendation or not.
These two variants share identical architectures, representation learning strategies, and training protocols.
The only difference lies in whether source-domain preference distributions are transferred to the target domain via optimal transport
in the last stage of our framework.
Therefore, performance differences can be directly attributed to the effect of cross-domain information.

To address \textbf{RQ2}, which examines the effect of distribution-based user preference modeling,
we compare DUP-OT (w/o source) with representative single-domain recommendation models,
namely LightGCN~\cite{He_Deng_Wang_Li_Zhang_Wang_2020_LightGCN} and NeuMF~\cite{He_Liao_Zhang_Nie_Hu_Chua_2017_NCF}.
In this setting, DUP-OT (w/o source) does not leverage any source-domain interactions and is trained exclusively on the target-domain training set.
Despite this, the only difference between DUP-OT (w/o source) and the baselines is the way of user preference modeling.
Our model represents user preferences as GMMs, while the baselines use point embeddings.
Thus, performance differences can be directly attributed to the effect of distribution-based user preference modeling.

Finally, to answer \textbf{RQ3}, which compares our proposed method with existing non-overlapping CDR models,
we select a representative non-overlapping CDR baseline model, TDAR~\cite{Yu_Lin_Ge_Ou_Qin_2020_TDAR},
and compare it with our full model, DUP-OT (w/ source), in the same experimental settings.

\subsection{Baseline Models}

As discussed in the previous section, we select different baseline models
to answer the proposed research questions from complementary perspectives.
In particular, the selected baselines cover both single-domain
recommendation models and representative non-overlapping cross-domain
recommendation approaches.
In total, we consider three baseline models and two variants of our
proposed framework in the experiments, which are described as follows:
\begin{itemize}

	\item \textbf{LightGCN}~\cite{He_Deng_Wang_Li_Zhang_Wang_2020_LightGCN}:
	      LightGCN is a representative graph-based collaborative filtering model
	      that learns user and item representations by propagating embeddings on
	      the user--item interaction graph.
	      Unlike earlier GCN-based recommenders, LightGCN removes feature
	      transformation and non-linear activation, and retains only neighborhood
	      aggregation, resulting in a simple yet effective architecture.
	      As a strong single-domain baseline, LightGCN represents user preferences
	      as deterministic embedding vectors and relies solely on target-domain
	      interaction data.
	      We include LightGCN to evaluate whether distribution-based user
	      preference modeling provides advantages over state-of-the-art
	      vector-based graph collaborative filtering methods.

	\item \textbf{NeuMF}~\cite{He_Liao_Zhang_Nie_Hu_Chua_2017_NCF}:
	      NeuMF is a classical neural collaborative filtering model that unifies
	      matrix factorization and multi-layer perceptrons under a neural
	      framework.
	      By replacing the inner product with a non-linear interaction function,
	      NeuMF aims to capture complex user--item interaction patterns in the
	      latent space.
	      Similar to LightGCN, NeuMF represents users and items as point-valued
	      latent vectors and operates in a purely single-domain setting.
	      This baseline is adopted to compare our distribution-based user
	      preference modeling with a widely used neural vector-based
	      recommendation paradigm.

	\item \textbf{TDAR}~\cite{Yu_Lin_Ge_Ou_Qin_2020_TDAR}:
	      TDAR is a representative non-overlapping cross-domain recommendation
	      model based on text-enhanced domain adaptation.
	      In the absence of shared users or items across domains, TDAR leverages
	      domain-invariant textual features extracted from user reviews as anchor
	      points to align latent spaces via adversarial training.
	      By transferring distribution patterns learned from a dense source
	      domain to a sparse target domain, TDAR alleviates data sparsity in
	      non-overlapping scenarios.
	      We include TDAR as a strong non-overlapping CDR baseline to compare
	      our optimal transport-based preference transfer mechanism with
	      existing text-driven domain adaptation approaches.

	\item \textbf{DUP-OT (w/ source)}:
	      This variant corresponds to the full version of our proposed
	      \textbf{DUP-OT} framework.
	      It models user preferences as Gaussian mixture distributions and
	      transfers such distributional preferences from the source domain to
	      the target domain via optimal transport.
	      This setting evaluates the complete effectiveness of distribution-based
	      preference modeling combined with cross-domain knowledge transfer.

	\item \textbf{DUP-OT (w/o source)}:
	      This variant disables the cross-domain transfer component and trains
	      the model using only target-domain data.
	      Under this setting, DUP-OT degenerates into a single-domain
	      recommendation model that still represents user preferences as
	      probability distributions.
	      By comparing this variant with vector-based single-domain baselines,
	      we can isolate and analyze the impact of distribution-based user
	      preference modeling independent of cross-domain information.

\end{itemize}

\subsection{Evaluation Metrics}

To quantitatively assess the recommendation performance of different models,
we formulate the recommendation task as a \emph{rating prediction} problem
and employ two widely used evaluation metrics: Root Mean Square Error (RMSE)
and Mean Absolute Error (MAE).

We adopt rating prediction as the evaluation task for two main reasons.
First, rating prediction provides a fine-grained and direct measure of how
accurately a model captures user preferences, which is particularly important
in cross-domain recommendation scenarios where the goal is to transfer
preference knowledge from a source domain to a target domain.
Second, in non-overlapping cross-domain recommendation settings, ranking-based
evaluation protocols often rely on strong assumptions about negative sampling
and candidate item sets, which may introduce additional bias.
In contrast, rating prediction allows us to evaluate model performance on
observed user--item interactions without requiring assumptions about
unobserved entries.

Following prior cross-domain recommendation studies that focus on rating
prediction under non-overlapping settings, we adopt RMSE and MAE as our
primary evaluation metrics.
Both metrics have been widely used in existing cross-domain recommendation
work and provide complementary perspectives on prediction accuracy.
RMSE penalizes large prediction errors more heavily and is therefore sensitive
to outliers, while MAE measures the average absolute deviation and offers a
more robust assessment of overall prediction quality.

Formally, given a set of test user--item interactions
$\mathcal{D}_{test} = \{(u, i, r_{ui})\}$,
where $r_{ui}$ denotes the ground-truth rating of user $u$ on item $i$,
the RMSE and MAE are defined as follows:
\begin{equation}
	\text{RMSE} = \sqrt{\frac{1}{|\mathcal{D}_{test}|}
		\sum_{(u, i, r_{ui}) \in \mathcal{D}_{test}} (\hat{r}_{ui} - r_{ui})^2}
\end{equation}
\begin{equation}
	\text{MAE} = \frac{1}{|\mathcal{D}_{test}|}
	\sum_{(u, i, r_{ui}) \in \mathcal{D}_{test}} |\hat{r}_{ui} - r_{ui}|
\end{equation}
where $\hat{r}_{ui}$ denotes the predicted rating generated by a recommendation
model.
Lower values of RMSE and MAE indicate better recommendation accuracy.

\section{Implementation Details}
In this section, we provide implementation details of our proposed \textbf{DUP-OT} framework
to ensure reproducibility of our experimental results.

We implement our model using PyTorch and conduct experiments on a machine with an NVIDIA RTX 3080 GPU.
To retrieve user and item embeddings from the interaction data, we utilize a pre-trained BERT model to encode from review texts.
Specifically, we use the \texttt{all-MiniLM-L6-v2} variant from the Sentence Transformers library~\cite{Reimers_Gurevych_2019_SBERT},
which produces 384-dimensional embeddings.

To